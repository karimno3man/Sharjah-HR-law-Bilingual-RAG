import re
import numpy as np
import faiss
from dataclasses import dataclass
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from openai import OpenAI


@dataclass
class Chunk:
    content: str
    metadata: Dict


# ========== NEW CHUNKING FUNCTIONS (INTEGRATED) ==========

def normalize_arabic_indic(text: str) -> str:
    """Convert Arabic-Indic numerals to Western numerals"""
    arabic_indic = 'Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©'
    western = '0123456789'
    translation = str.maketrans(arabic_indic, western)
    return text.translate(translation)


def is_article_header(line: str) -> bool:
    """Determine if a line is an article header (not a reference)"""
    stripped = line.strip()
    
    # Too long to be a header
    if len(stripped) > 200:
        return False
    
    # Skip obvious references - handle both "Ø§Ù„Ù…Ø§Ø¯Ø©" and "Ø§Ù„Ù…Ø§ Ø¯Ø©" (with space)
    reference_patterns = [
        'Ø¨Ø§Ù„Ù…Ø§Ø¯Ø©', 'Ù…Ù† Ø§Ù„Ù…Ø§Ø¯Ø©', 'Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø§Ø¯Ø©',
        'ÙÙŠ Ø§Ù„Ù…Ø§Ø¯Ø©', 'Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø§Ø¯Ø©', 'ÙˆÙÙ‚ Ø§Ù„Ù…Ø§Ø¯Ø©',
        'Ø§Ù„Ù…Ø§Ø¯Ø© Ø±Ù‚Ù…', 'Ù†Øµ Ø§Ù„Ù…Ø§Ø¯Ø©',
        'Ø¨Ø§Ù„Ù…Ø§ Ø¯Ø©', 'Ù…Ù† Ø§Ù„Ù…Ø§ Ø¯Ø©', 'Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø§ Ø¯Ø©',
        'ÙÙŠ Ø§Ù„Ù…Ø§ Ø¯Ø©', 'Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø§ Ø¯Ø©', 'ÙˆÙÙ‚ Ø§Ù„Ù…Ø§ Ø¯Ø©',
        'Ø§Ù„Ù…Ø§ Ø¯Ø© Ø±Ù‚Ù…', 'Ù†Øµ Ø§Ù„Ù…Ø§ Ø¯Ø©'
    ]
    
    for pattern in reference_patterns:
        if pattern in stripped:
            return False
    
    # Check if it matches article header patterns
    # Handle both "Ø§Ù„Ù…Ø§Ø¯Ø©" and "Ø§Ù„Ù…Ø§ Ø¯Ø©" (with space between Ø§Ù„ and Ù…Ø§Ø¯Ø©)
    if re.match(r'^Ø§Ù„Ù…Ø§?\s*Ø¯Ø©\s*[\(\[]?\s*\d+', stripped):
        return True
    
    if re.match(r'^[\(\[]?[^\(\)\[\]]*\d+\s*[\)\]]\s*Ø§Ù„Ù…Ø§?\s*Ø¯Ø©', stripped):
        return True
    
    if stripped.endswith(('Ø§Ù„Ù…Ø§Ø¯Ø©', 'Ø§Ù„Ù…Ø§ Ø¯Ø©')) and re.search(r'\d+', stripped):
        return True
    
    return False


def is_table_header(line: str) -> bool:
    """Determine if a line is a table header"""
    stripped = line.strip()
    
    # Check for various table header patterns:
    # - ## Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù… (X)
    # - ### Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù… (X)
    # - ### **Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù… (X)**
    # - **Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù… (X):**
    # - **Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù… (X):**
    # - Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù… (X):
    # - Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù… (X) (continuations)
    
    # Pattern 1: Markdown headers (## or ###) followed by table reference
    if re.match(r'^#{2,3}\s*(\*\*)?.*?(Ø§Ù„)?Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù…\s*\(', stripped):
        return True
    
    # Pattern 2: Bold markdown table headers
    if re.match(r'^\*\*.*?(Ø§Ù„)?Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù…\s*\(', stripped):
        return True
    
    # Pattern 3: Plain table headers starting with Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù…
    if re.match(r'^(Ø§Ù„)?Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù…\s*\(', stripped):
        # Skip references to tables (not actual table headers)
        if any(pattern in stripped for pattern in ['Ø¨Ø§Ù„Ø¬Ø¯ÙˆÙ„', 'Ù…Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„', 'ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„', 'Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø¯ÙˆÙ„', 'Ù„Ù„Ø¬Ø¯ÙˆÙ„', 'ÙˆÙÙ‚ Ø§Ù„Ø¬Ø¯ÙˆÙ„', 'ÙˆÙÙ‚Ø§ Ù„Ù„Ø¬Ø¯ÙˆÙ„']):
            return False
        return True
    
    # Pattern 4: Table continuations (Ø§Ø³ØªÙƒÙ…Ø§Ù„)
    if 'Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø¬Ø¯ÙˆÙ„ Ø±Ù‚Ù…' in stripped:
        return True
    
    return False


def chunk_by_article_and_table(text: str, normalize_numerals: bool = True) -> List[Chunk]:
    """
    Chunk Arabic legal document by articles (Ø§Ù„Ù…Ø§Ø¯Ø©) and tables (Ø¬Ø¯ÙˆÙ„).
    
    Handles formats:
    - Ø§Ù„Ù…Ø§Ø¯Ø© (5), (5) Ø§Ù„Ù…Ø§Ø¯Ø©, )text 5( Ø§Ù„Ù…Ø§Ø¯Ø©
    - Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª, Ø¬Ø¯ÙˆÙ„ ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ù‡Ù†Ø¯Ø³ÙŠÙ†
    """
    
    # Normalize Arabic-Indic numerals
    if normalize_numerals:
        text = normalize_arabic_indic(text)
    
    lines = text.split('\n')
    chunk_starts = []
    
    for i, line in enumerate(lines):
        if is_article_header(line):
            chunk_starts.append((i, line.strip(), 'article'))
        elif is_table_header(line):
            chunk_starts.append((i, line.strip(), 'table'))
    
    # Create chunks
    chunks = []
    
    for idx, (line_num, header, chunk_type) in enumerate(chunk_starts):
        start_line = line_num
        end_line = chunk_starts[idx + 1][0] if idx + 1 < len(chunk_starts) else len(lines)
        
        chunk_lines = lines[start_line:end_line]
        chunk_text = '\n'.join(chunk_lines).strip()
        
        # Extract number
        number_match = re.search(r'\d+', header)
        number = number_match.group() if number_match else None
        
        # Calculate positions
        start_pos = sum(len(l) + 1 for l in lines[:start_line])
        end_pos = start_pos + len(chunk_text)
        
        chunks.append(
            Chunk(
                content=chunk_text,
                metadata={
                    'type': chunk_type,
                    'header': header,
                    'number': number,
                    'chunk_size': len(chunk_text),
                    'start_line': start_line,
                    'end_line': end_line,
                    'start_pos': start_pos,
                    'end_pos': end_pos,
                    'source': 'Legal_Document'
                }
            )
        )
    
    return chunks


def sub_chunk_large_articles(chunks: List[Chunk], max_size: int) -> List[Chunk]:
    """Split large chunks into smaller sub-chunks"""
    new_chunks = []
    
    for chunk in chunks:
        if chunk.metadata['chunk_size'] <= max_size:
            new_chunks.append(chunk)
        else:
            # Split by paragraphs
            paragraphs = chunk.content.split('\n')
            
            sub_chunk_content = ""
            sub_chunk_index = 1
            
            for para in paragraphs:
                if len(sub_chunk_content) + len(para) > max_size and sub_chunk_content:
                    new_chunks.append(
                        Chunk(
                            content=sub_chunk_content.strip(),
                            metadata={
                                **chunk.metadata,
                                'subsection': f'part_{sub_chunk_index}',
                                'chunk_size': len(sub_chunk_content),
                                'is_subchunk': True,
                                'original_size': chunk.metadata['chunk_size']
                            }
                        )
                    )
                    sub_chunk_content = para + "\n"
                    sub_chunk_index += 1
                else:
                    sub_chunk_content += para + "\n"
            
            # Add the last sub-chunk
            if sub_chunk_content.strip():
                new_chunks.append(
                    Chunk(
                        content=sub_chunk_content.strip(),
                        metadata={
                            **chunk.metadata,
                            'subsection': f'part_{sub_chunk_index}',
                            'chunk_size': len(sub_chunk_content),
                            'is_subchunk': True,
                            'original_size': chunk.metadata['chunk_size']
                        }
                    )
                )
    
    return new_chunks


def normalize_arabic_text(text: str) -> str:
    """Normalize Arabic text"""
    text = re.sub(r'\s+', ' ', text)
    text = text.replace('Ù€', '')
    return text.strip()


# ========== RAG ENGINE (UPDATED WITH NEW CHUNKING) ==========

class RAGEngine:
    def __init__(self, document_path: str, openai_api_key: str, max_chunk_size: int = 5000):
        """Initialize RAG engine with document and API key"""
        print("Loading and chunking document...")
        
        # Load document
        with open(document_path, "r", encoding="utf-8") as f:
            raw_text = f.read()
        
        # NEW: Use the improved chunking function
        self.chunks = chunk_by_article_and_table(raw_text, normalize_numerals=True)
        
        # Normalize text in chunks
        for c in self.chunks:
            c.content = normalize_arabic_text(c.content)
        
        # NEW: Sub-chunk large articles
        if max_chunk_size:
            print(f"Sub-chunking articles larger than {max_chunk_size} characters...")
            original_count = len(self.chunks)
            self.chunks = sub_chunk_large_articles(self.chunks, max_chunk_size)
            print(f"Chunks: {original_count} â†’ {len(self.chunks)} (after sub-chunking)")
        
        # Print chunking summary
        articles = sum(1 for c in self.chunks if c.metadata['type'] == 'article')
        tables = sum(1 for c in self.chunks if c.metadata['type'] == 'table')
        print(f"Total chunks: {len(self.chunks)} ({articles} articles, {tables} tables)")
        
        # Initialize embedder and FAISS
        print("Creating embeddings...")
        self.embedder = SentenceTransformer("intfloat/multilingual-e5-large")
        
        texts = [c.content for c in self.chunks]
        passage_texts = [f"passage: {text}" for text in texts]
        embeddings = self.embedder.encode(passage_texts, normalize_embeddings=True)
        
        dim = embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dim)
        self.faiss_index.add(np.array(embeddings))
        
        self.id2chunk = {i: c for i, c in enumerate(self.chunks)}
        
        # Initialize BM25
        print("Initializing BM25...")
        tokenized_corpus = [c.content.split() for c in self.chunks]
        self.bm25 = BM25Okapi(tokenized_corpus)
        
        # Initialize OpenAI client
        self.client = OpenAI(api_key=openai_api_key)
        
        # Conversation memory (current session only)
        self.conversation_history: List[Dict] = []
        self.max_history_turns = 6  # Keep last 6 turns (3 Q&A pairs)
        
        print("âœ… RAG engine ready!")
    
    def detect_language(self, text: str) -> str:
        """Simple language detection based on Arabic characters"""
        arabic_chars = len(re.findall(r'[\u0600-\u06FF]', text))
        total_chars = len(re.sub(r'\s', '', text))
        
        if total_chars == 0:
            return "en"
        
        return "ar" if (arabic_chars / total_chars) > 0.3 else "en"
    
    def translate_to_arabic(self, text: str) -> str:
        """Translate English text to Arabic using GPT-4o-mini"""
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system", 
                    "content": "You are a translator. Translate the following text to Arabic. Output ONLY the Arabic translation, no explanations or additional text."
                },
                {
                    "role": "user", 
                    "content": text
                }
            ],
            temperature=0,
            max_tokens=1000
        )
        return response.choices[0].message.content.strip()
    
    def rewrite_query_with_history(self, query: str) -> str:
        """
        If there's conversation history, rewrite the query as a fully standalone question.
        This ensures follow-up questions like 'what about exceptions?' retrieve correctly.
        """
        if not self.conversation_history:
            return query  # No history yet, use query as-is
        
        # Build a compact history string
        history_text = ""
        for turn in self.conversation_history[-self.max_history_turns:]:
            role = "User" if turn["role"] == "user" else "Assistant"
            history_text += f"{role}: {turn['content']}\n"
        
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are a query rewriter. Given a conversation history and a follow-up question, "
                        "rewrite the follow-up question as a single, fully self-contained question "
                        "that includes all necessary context from the history. "
                        "Output ONLY the rewritten question, nothing else. "
                        "Keep the same language as the follow-up question."
                    )
                },
                {
                    "role": "user",
                    "content": f"Conversation history:\n{history_text}\nFollow-up question: {query}\n\nRewritten standalone question:"
                }
            ],
            temperature=0,
            max_tokens=200
        )
        rewritten = response.choices[0].message.content.strip()
        return rewritten

    def clear_history(self):
        """Clear conversation history to start a fresh session."""
        self.conversation_history = []
        print("ğŸ—‘ï¸ Conversation history cleared.")

    def retrieve_chunks(self, query: str, top_k=5, k=60):
        """
        Retrieve relevant chunks using Reciprocal Rank Fusion (RRF)
        
        RRF combines rankings from FAISS and BM25 using the formula:
        RRF_score(d) = sum over all rankings( 1 / (k + rank(d)) )
        
        Args:
            query: Search query
            top_k: Number of chunks to return
            k: RRF constant (default 60, typical range 30-100)
        """
        # FAISS semantic search
        query_text = f"query: {query}"
        q_emb = self.embedder.encode([query_text], normalize_embeddings=True)
        
        # Get more candidates for better fusion
        search_k = min(top_k * 2, len(self.chunks))
        faiss_scores, faiss_ids = self.faiss_index.search(q_emb, search_k)

        # Convert FAISS results to Python native types and filter out invalid indices (-1)
        faiss_ids_list = [int(idx) for idx in faiss_ids[0] if idx >= 0]

        # BM25 keyword search
        bm25_scores = self.bm25.get_scores(query.split())
        bm25_ranking = sorted(
            range(len(bm25_scores)),
            key=lambda i: bm25_scores[i],
            reverse=True
        )[:search_k]

        # RRF: Reciprocal Rank Fusion
        rrf_scores = {}
        
        # Add FAISS rankings to RRF
        for rank, idx in enumerate(faiss_ids_list):
            if idx in self.id2chunk:
                rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (k + rank + 1)
        
        # Add BM25 rankings to RRF
        for rank, idx in enumerate(bm25_ranking):
            if idx in self.id2chunk:
                rrf_scores[idx] = rrf_scores.get(idx, 0) + 1 / (k + rank + 1)
        
        # Handle case where no chunks were retrieved
        if not rrf_scores:
            return []
        
        # Sort by RRF score (higher is better)
        sorted_chunks = sorted(
            rrf_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # Return top-k chunks
        return [
            self.id2chunk[idx] 
            for idx, score in sorted_chunks[:top_k]
            if idx in self.id2chunk
        ]
    
    def answer_question(self, query: str, debug=False):
        """Answer a question using RAG with conversation memory."""
        # Detect original language
        original_lang = self.detect_language(query)

        # Step 1: Rewrite query using conversation history for better retrieval
        standalone_query = self.rewrite_query_with_history(query)
        if debug and standalone_query != query:
            print(f"[DEBUG] Original query: '{query}'")
            print(f"[DEBUG] Rewritten query: '{standalone_query}'")

        # Translate to Arabic if needed for better retrieval
        if original_lang == "en":
            if debug:
                print(f"[DEBUG] English query detected: '{standalone_query}'")
            arabic_query = self.translate_to_arabic(standalone_query)
            if debug:
                print(f"[DEBUG] Translated to Arabic: '{arabic_query}'")
            retrieval_query = arabic_query
        else:
            retrieval_query = standalone_query

        # Retrieve chunks
        retrieved = self.retrieve_chunks(retrieval_query, top_k=10)
        
        # Handle case where no chunks were retrieved
        if not retrieved:
            if original_lang == "ar":
                return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©."
            else:
                return "Sorry, I couldn't find relevant information for your question in the document."
        
        if debug:
            print(f"\n[DEBUG] Retrieved {len(retrieved)} chunks:")
            for i, c in enumerate(retrieved[:5], 1):
                subsection = f" ({c.metadata.get('subsection')})" if c.metadata.get('subsection') else ""
                print(f"  {i}. {c.metadata['header']}{subsection}")
                print(f"     Size: {c.metadata['chunk_size']} chars")
        
        # Build context
        context = "\n\n".join(
            f"[{c.metadata['header']}]\n{c.content}"
            for c in retrieved
        )
        
        # Generate answer in original language
        if original_lang == "ar":
            system_msg = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ.
            Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙÙ‚Ø·.
            Ù„Ø§ ØªÙØªØ±Ø¶ ÙˆÙ„Ø§ ØªØ¶Ù Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Øµ.

            ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ:
            - Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù†ÙØ³ Ù„ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ ØªÙ…Ø§Ù…Ø§Ù‹ (Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·).
            - Ø¹Ø¯Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ù„ØºØ© Ø£Ø®Ø±Ù‰ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©.

            Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØ·Ù„Ø¨ Ø¹Ù†Ø§ØµØ± Ù…ØªØ¹Ø¯Ø¯Ø© (Ù†Ù‚Ø§Ø·ØŒ Ø´Ø±ÙˆØ·ØŒ ÙØ¦Ø§ØªØŒ Ø±ÙˆØ§ØªØ¨ØŒ Ø¥Ù„Ø®) ÙÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ:
            - Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø© ÙƒØ§Ù…Ù„Ø© Ø¨Ø¯ÙˆÙ† Ø­Ø°Ù Ø£ÙŠ Ù†Ù‚Ø·Ø©.
            - Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø± Ø£ÙŠ Ù†Ù‚Ø·Ø©.
            - Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø© ÙƒÙ†Ù‚Ø§Ø· ÙÙŠ Ø§Ù„Ù†Øµ.

           """

            user_prompt = f"""Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:

            {context}

             Ø§Ù„Ø³Ø¤Ø§Ù„:
            {query}

            ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:
            - Ø§Ù‚Ø±Ø£ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø¹Ù†Ø§ÙŠØ©.
            - Ø§Ø³ØªØ®Ø±Ø¬ ÙƒÙ„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„.
            - Ù„Ø§ ØªØ­Ø°Ù Ø£ÙŠ Ø¨Ù†Ø¯ Ù…Ø°ÙƒÙˆØ± ÙÙŠ Ø§Ù„Ù†Øµ.
            - Ù„Ø§ ØªÙƒØ±Ø± Ø£ÙŠ Ø¨Ù†Ø¯.
            - Ù„Ø§ ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ø§Ù…ØŒ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ø­Ø±ÙÙŠ Ø£Ø¹Ù„Ø§Ù‡.
            - ÙÙŠ Ø­Ø§Ù„ Ø§Ø®ØªÙ„Ø§Ù ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù†Øµ (Ù…Ø«Ù„ Ø§Ø®ØªÙ„Ø§Ù Ø¨Ø³ÙŠØ· ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª)ØŒ Ø§Ø¹ØªØ¨Ø± Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ù…Ù‚ØµÙˆØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙˆØ§Ø¶Ø­Ø§Ù‹ Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚.
            - ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù†ÙØ³ Ù„ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙ‚Ø·.

            Ø£Ø¬Ø¨ Ø§Ù„Ø¢Ù†."""

        else:
            system_msg = """You are a professional legal assistant.
            Your task is to extract ALL relevant information strictly from the provided texts.
            Do NOT add information from outside the texts.

            CRITICAL LANGUAGE RULE: The source texts are in Arabic, but the user's question is in English.
            You MUST translate all relevant information from Arabic to English and answer ENTIRELY in English.
            Do NOT include any Arabic text in your response. Every word of your answer must be in English.

            If the question requires multiple items (bullets, conditions, categories, salaries, etc.), you MUST:
            - Retrieve all listed items completely.
            - Do not omit any item mentioned in the text.
            - Do not repeat any item.
            - Preserve bullet structure if present in the text.

            """

            user_prompt = f"""The following legal texts are in Arabic. Read them carefully and answer the question in English only.

            Arabic legal texts:
            {context}

            Question (in English â€” your answer MUST also be in English):
            {query}

            Important instructions:
            - Carefully review all provided Arabic texts.
            - Extract every relevant item fully and translate it to English.
            - Do not omit any listed element.
            - Do not duplicate any element.
            - Base your answer strictly on explicit text.
            - If there is minor wording variation or typo in the question (e.g. spelling differences), interpret it according to the closest matching term in the provided text.
            - YOUR ENTIRE ANSWER MUST BE IN ENGLISH. Do not use any Arabic in your response.

            Provide your final answer now in English."""

        messages = [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_prompt}
        ]

        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.3,
            max_tokens=1500
        )
        
        answer = response.choices[0].message.content

        # Save this turn to conversation history
        self.conversation_history.append({"role": "user", "content": standalone_query})
        self.conversation_history.append({"role": "assistant", "content": answer})

        # Trim history to max window
        if len(self.conversation_history) > self.max_history_turns:
            self.conversation_history = self.conversation_history[-self.max_history_turns:]

        return answer
    
    def get_statistics(self):
        """Get statistics about the chunks"""
        total_chunks = len(self.chunks)
        articles = sum(1 for c in self.chunks if c.metadata['type'] == 'article')
        tables = sum(1 for c in self.chunks if c.metadata['type'] == 'table')
        sub_chunks = sum(1 for c in self.chunks if c.metadata.get('is_subchunk'))
        
        sizes = [c.metadata['chunk_size'] for c in self.chunks]
        
        # Get unique article numbers
        article_numbers = sorted(set(
            int(c.metadata['number']) 
            for c in self.chunks 
            if c.metadata['type'] == 'article' and c.metadata['number']
        ))
        
        return {
            'total_chunks': total_chunks,
            'articles': articles,
            'tables': tables,
            'sub_chunks': sub_chunks,
            'unique_article_numbers': len(article_numbers),
            'article_range': f"{min(article_numbers)} to {max(article_numbers)}" if article_numbers else "N/A",
            'avg_size': sum(sizes) / len(sizes) if sizes else 0,
            'max_size': max(sizes) if sizes else 0,
            'min_size': min(sizes) if sizes else 0
        }


# ========== EXAMPLE USAGE ==========

if __name__ == "__main__":
    import os
    
    # Initialize RAG engine
    engine = RAGEngine(
        document_path="arabic_text_and_tables.txt",
        openai_api_key=os.getenv("OPENAI_API_KEY"),
          # Adjust as needed
    )
    
    # Print statistics
    stats = engine.get_statistics()
    print("\n" + "="*80)
    print("STATISTICS")
    print("="*80)
    for key, value in stats.items():
        print(f"{key}: {value}")
    
    # Test queries
    print("\n" + "="*80)
    print("EXAMPLE QUERIES")
    print("="*80)
    
    # Arabic query
    print("\n1. Arabic query:")
    answer = engine.answer_question("Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø¯ÙˆØ±ÙŠØ© Ù„Ù„Ù…ÙˆØ¸ÙØŸ", debug=True)
    print(f"\nAnswer: {answer}")
    
    # English query
    print("\n\n2. English query:")
    answer = engine.answer_question("What is the annual leave entitlement?", debug=True)
    print(f"\nAnswer: {answer}")